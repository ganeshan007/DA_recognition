{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.24.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.51.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "!pip install torchtext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove_embeddings=GloVe(name='6B',dim='50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'-\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "print(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 100, 44])\n"
     ]
    }
   ],
   "source": [
    "# inp=torch.rand(20,16,50)\n",
    "# word=['w','o','r','d']\n",
    "# char=nn.Conv1d(16,100,7)\n",
    "# out=char(inp)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              swda_filename ptb_basename  conversation_no  transcript_index  \\\n",
      "0  sw00utt/sw_0001_4325.utt     4/sw4325             4325                 0   \n",
      "1  sw00utt/sw_0001_4325.utt     4/sw4325             4325                 1   \n",
      "2  sw00utt/sw_0001_4325.utt     4/sw4325             4325                 2   \n",
      "3  sw00utt/sw_0001_4325.utt     4/sw4325             4325                 3   \n",
      "4  sw00utt/sw_0001_4325.utt     4/sw4325             4325                 4   \n",
      "\n",
      "  act_tag                      act_label_1 act_label_2 act_label_relation  \\\n",
      "0       o                            Other        none               none   \n",
      "1      qw         Info-request:Wh-Question        none               none   \n",
      "2    qy^d     Info-request:Yes-No-Question        none               none   \n",
      "3       +  Other:Segment-(multi-utterance)        none               none   \n",
      "4       +  Other:Segment-(multi-utterance)        none               none   \n",
      "\n",
      "  caller utterance_index subutterance_index  \\\n",
      "0      A               1                  1   \n",
      "1      A               1                  2   \n",
      "2      B               2                  1   \n",
      "3      A               3                  1   \n",
      "4      B               4                  1   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0                                              Okay.   \n",
      "1                                                 So   \n",
      "2                                            I guess   \n",
      "3  What kind of experience do you, do you have, t...   \n",
      "4              I think, uh, I wonder if that worked.   \n",
      "\n",
      "                                                   X topic_description  \\\n",
      "0                      (ROOT (ADJP (JJ okay) (. .)))        CHILD CARE   \n",
      "1                              (ROOT (ADVP (RB so)))        CHILD CARE   \n",
      "2         (ROOT (FRAG (LST (LS i)) (NP (NN guess))))        CHILD CARE   \n",
      "3  (ROOT (SQ (SBAR (WHNP (WHNP (WDT what) (NN kin...        CHILD CARE   \n",
      "4  (ROOT (S (VP (LS i) (S (VP (VP (VB think)) (, ...        CHILD CARE   \n",
      "\n",
      "                                              prompt  \n",
      "0  FIND OUT WHAT CRITERIA THE OTHER CALLER WOULD ...  \n",
      "1  FIND OUT WHAT CRITERIA THE OTHER CALLER WOULD ...  \n",
      "2  FIND OUT WHAT CRITERIA THE OTHER CALLER WOULD ...  \n",
      "3  FIND OUT WHAT CRITERIA THE OTHER CALLER WOULD ...  \n",
      "4  FIND OUT WHAT CRITERIA THE OTHER CALLER WOULD ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df_train=pd.read_csv('switchboard_train.csv')\n",
    "print(df_train.head())\n",
    "text_field = Field(\n",
    "    tokenize='basic_english', \n",
    "    lower=True)\n",
    "label_field = Field(sequential=False, use_vocab=True)\n",
    "preprocessed_text = df_train['clean_text'].apply(lambda x: text_field.preprocess(str(x)))\n",
    "text_field.build_vocab(\n",
    "    preprocessed_text, \n",
    "    vectors='glove.6B.50d')\n",
    "# vocab=text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('switchboard_complete.csv')\n",
    "df_train=df[:160000]\n",
    "df_valid=df[160000:200000]\n",
    "df_test=df[200000:]\n",
    "df_train=df_train.fillna('')\n",
    "df_valid=df_valid.fillna('')\n",
    "df_test=df_test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('switchboard_train.csv',index=False)\n",
    "# df_valid.to_csv('switchboard_valid.csv',index=False)\n",
    "df_test.to_csv('switchboard_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "train_datafields=[('swda_filename',None),('ptb_basename',None),('conversation_no',None),('transcript_index',None),('act_tag',None),('act_label_1',label_field),('act_label_2',None),('act_label_relation',None),('caller',None),('utterance_index',None),('subutterance_index',None),('clean_text',text_field),('topic_description',None),('prompt',None)]\n",
    "train,validation,test=TabularDataset.splits(path='',train='switchboard_train.csv',validation='switchboard_valid.csv',test='switchboard_test.csv',format='csv',skip_header=True,fields=train_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['act_label_1', 'clean_text'])\n"
     ]
    }
   ],
   "source": [
    "train[0].__dict__.keys()\n",
    "print(test[0].__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(train,vectors='glove.6B.50d')\n",
    "label_field.build_vocab(train,vectors='glove.6B.50d')\n",
    "# text_field.vocab.stoi['are']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocabulary=text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18618\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vocabulary.vectors))\n",
    "print(len(label_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# BATCH_SIZE = 128\n",
    "batch_size=80\n",
    "\n",
    "\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits((train, validation),batch_sizes=(batch_size,batch_size),device=device,sort_key=lambda x: len(x.clean_text),sort_within_batch=False,repeat=False)\n",
    "test_iter = Iterator(test, batch_size=batch_size, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,da,u_2,r,k): #keep r and u_2 same\n",
    "        super(Attention,self).__init__()\n",
    "        self.ws1=torch.nn.Linear(u_2,da,bias=False)\n",
    "        self.ws3=torch.nn.Linear(k,da,bias=True)\n",
    "#         self.tanh=torch.tanh()\n",
    "        self.ws2=torch.nn.Linear(da,r,bias=False)\n",
    "        self.projection=nn.Linear(batch_size,50)\n",
    "    def forward(self,utt_hid,conv_hid):\n",
    "        lin1=self.ws1(utt_hid)\n",
    "        lin2=self.ws3(conv_hid)\n",
    "        add=torch.add(lin1,lin2)\n",
    "        squash=torch.tanh(add)\n",
    "        s=self.ws2(squash)\n",
    "        attention=torch.softmax(s,dim=1)\n",
    "#         print(attention.shape)\n",
    "        scores=torch.matmul(attention,utt_hid.T)\n",
    "        scores=self.projection(scores).flatten()\n",
    "        return scores\n",
    "    \n",
    "class ConversationRNN(nn.Module):\n",
    "    def __init__(self,embedding_dim,hidden_dim,num_layers):\n",
    "        super(ConversationRNN,self).__init__()\n",
    "        self.gru=nn.GRU(embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,bidirectional=True)\n",
    "    def forward(self,x):\n",
    "        out,hid=self.gru(x)\n",
    "        forward_hid=hid.view(self.gru.num_layers,2,x.shape[1],self.gru.hidden_size)[0,0,0]\n",
    "        return forward_hid,out,hid\n",
    "    \n",
    "class UtteranceRNN(nn.Module):\n",
    "    def __init__(self,embedding,embedding_dim=50,hidden_dim=100,num_layers=1):\n",
    "        super(UtteranceRNN,self).__init__()\n",
    "        self.embedding=embedding\n",
    "        self.gru=nn.GRU(embedding_dim,hidden_dim,num_layers,bidirectional=True)\n",
    "    def forward(self,x):\n",
    "        embed=self.embedding[x]\n",
    "        out,hid=self.gru(embed)\n",
    "        return hid.view(x.shape[1],-1)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_accuracy(true_labels,predictions):\n",
    "    classes=torch.argmax(predictions,dim=1)\n",
    "    return torch.mean((classes==true_labels).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  5 11:39:21 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro M4000        On   | 00000000:00:05.0 Off |                  N/A |\n",
      "| 46%   30C    P8    11W / 120W |      4MiB /  8126MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch_no. 0 and batch_no 0 is 2.722581148147583 and acc is 0.4000000059604645\n",
      "loss on epoch_no. 0 and batch_no 100 is 2.7997438293872494 and acc is 0.32500001788139343\n",
      "loss on epoch_no. 0 and batch_no 200 is 2.8046348094940186 and acc is 0.3125\n",
      "loss on epoch_no. 0 and batch_no 300 is 2.8050690291331852 and acc is 0.3499999940395355\n",
      "loss on epoch_no. 0 and batch_no 400 is 2.804427985241288 and acc is 0.26250001788139343\n",
      "loss on epoch_no. 0 and batch_no 500 is 2.8030011725282953 and acc is 0.42500001192092896\n",
      "loss on epoch_no. 0 and batch_no 600 is 2.801576667934805 and acc is 0.3499999940395355\n",
      "loss on epoch_no. 0 and batch_no 700 is 2.8050300530802335 and acc is 0.25\n",
      "loss on epoch_no. 0 and batch_no 800 is 2.8034858391079562 and acc is 0.3499999940395355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3f69068d038c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_label_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "batch_size=80\n",
    "# uttRNN=UtteranceRNN(word_vocabulary.vectors,50,100,1)\n",
    "# hid2labels=torch.nn.Linear(200,55)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "# att=Attention(da=300,u_2=200,r=200,k=100)\n",
    "# convRNN=ConversationRNN(batch_size*50,batch_size*100,1)\n",
    "params=list(uttRNN.parameters())+list(att.parameters())+list(convRNN.parameters())\n",
    "epochs=2\n",
    "optimizer=torch.optim.Adam(params)\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses=[]\n",
    "    epoch_acc=[]\n",
    "    for idx,batch in enumerate(train_iter):\n",
    "        conv_hid=torch.randn(1,batch_size*100)\n",
    "        optimizer.zero_grad()\n",
    "        utt_hid=uttRNN(batch.clean_text)\n",
    "        scores=att(utt_hid,conv_hid.view(batch_size,-1))\n",
    "        conv_hid,out,hid=convRNN(scores.view(1,1,-1))\n",
    "        predicted_labels=hid2labels(hid.view(batch_size,-1)).to(device)\n",
    "        accuracy=get_accuracy(batch.act_label_1,predicted_labels)\n",
    "        epoch_acc.append(accuracy)\n",
    "        loss=loss_fn(predicted_labels,batch.act_label_1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        if(idx%100==0):\n",
    "            print('loss on epoch_no. {} and batch_no {} is {} and acc is {}'.format(epoch,idx,np.mean(epoch_losses),accuracy))\n",
    "        \n",
    "#     val_losses=[]\n",
    "#     val_acc=[]\n",
    "#     for idx,batch in enumerate(val_iter):\n",
    "#         with torch.no_grad():\n",
    "#             optimizer.zero_grad()\n",
    "#             uttRNN.eval()\n",
    "#             att.eval()\n",
    "#             convRNN.eval()\n",
    "#             utt_hid=uttRNN(batch.clean_text)\n",
    "#             scores=att(utt_hid,conv_hid.view(batch_size,-1))\n",
    "#             conv_hid,out,hid=convRNN(scores.view(1,1,-1))\n",
    "#             labels=hid2labels(hid.view(batch_size,-1)).to(device)\n",
    "#             t_loss=loss_fn(labels,batch.act_label_1)\n",
    "#             val_accuracy=get_accuracy(batch.act_label_1,labels)\n",
    "#             val_acc.append(val_accuracy)\n",
    "#             val_losses.append(t_loss.item())\n",
    "#             if(idx%100==0):\n",
    "#                 print('test loss on epoch_no. {} and batch_ no {} is {} and val acc is {}'.format(epoch,idx,np.mean(val_losses),val_accuracy))\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "# embeddinggggg=word_vocabulary.vectors\n",
    "# lstm=UtteranceRNN(word_vocabulary.vectors,50,100,1)\n",
    "# for batch in train_iter:\n",
    "#     i+=1\n",
    "#     if(i<5):\n",
    "# #         batch.clean_text=batch.clean_text.view(64,-1)\n",
    "#         print(batch.clean_text.shape)\n",
    "# #         embed=embeddinggggg[batch.clean_text]\n",
    "#         hid=lstm(batch.clean_text)\n",
    "# #         pr1int(hid)\n",
    "#         print(hid.shape)\n",
    "#         scores=att(hid.view(160,-1),conv_hidden.view(160,-1))\n",
    "#         print(scores.shape)\n",
    "#         conv_hidden,out=convRNN(scores.view(1,1,-1))\n",
    "        \n",
    "# #         for j in range(batch.clean_text.shape[0]):\n",
    "# #             sent=batch.clean_text[j]\n",
    "# #             for word in sent:ṇ\n",
    "# #                 w=word_vocabulary.itos[word]\n",
    "# # #                 print(type(w))\n",
    "# #                 for char in w:\n",
    "# #                     print(all_letters[char])\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (80) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5e326206601c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mconvRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mutt_hid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muttRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_hid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mconv_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhid2labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-66b50736712c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, utt_hid, conv_hid)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlin1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_hid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlin2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_hid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0madd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0msquash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (80) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "test_losses=[]\n",
    "test_acc=[]\n",
    "preds=[]\n",
    "for idx,batch in enumerate(test_iter):\n",
    "    with torch.no_grad():\n",
    "        optimizer.zero_grad()\n",
    "        uttRNN.eval()\n",
    "        att.eval()\n",
    "        convRNN.eval()\n",
    "        utt_hid=uttRNN(batch.clean_text)\n",
    "        scores=att(utt_hid,conv_hid.view(batch_size,-1))\n",
    "        conv_hid,out,hid=convRNN(scores.view(1,1,-1))\n",
    "        labels=hid2labels(hid.view(batch_size,-1)).to(device)\n",
    "        preds.append(torch.argmax(labels,dim=1))\n",
    "        t_loss=loss_fn(labels,batch.act_label_1)\n",
    "        val_accuracy=get_accuracy(batch.act_label_1,labels)\n",
    "        test_acc.append(val_accuracy.item())\n",
    "        test_losses.append(t_loss.item())\n",
    "#         if(idx%100==0):\n",
    "#             print('test loss on epoch_no. {} and batch_ no {} is {} and val acc is {}'.format(epoch,idx,np.mean(test_losses),val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type UtteranceRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ConversationRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model=nn.Sequential(uttRNN,att,convRNN,hid2labels)\n",
    "torch.save(model,'Raheja_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): UtteranceRNN(\n",
      "    (gru): GRU(50, 100, bidirectional=True)\n",
      "  )\n",
      "  (1): Attention(\n",
      "    (ws1): Linear(in_features=200, out_features=300, bias=False)\n",
      "    (ws3): Linear(in_features=100, out_features=300, bias=True)\n",
      "    (ws2): Linear(in_features=300, out_features=200, bias=False)\n",
      "    (projection): Linear(in_features=80, out_features=50, bias=True)\n",
      "  )\n",
      "  (2): ConversationRNN(\n",
      "    (gru): GRU(4000, 8000, bidirectional=True)\n",
      "  )\n",
      "  (3): Linear(in_features=200, out_features=55, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "l=torch.load('Raheja_model.pth')\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}